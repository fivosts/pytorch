// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#define __STDC_FORMAT_MACROS

#include <ATen/MkldnnCPUType.h>

// @generated by aten/src/ATen/gen.py

#include <c10/core/TensorImpl.h>
#include <ATen/CPUGenerator.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NativeFunctions.h>
#ifdef BUILD_NAMEDTENSOR
#include <ATen/NamedTensorUtils.h>
#endif
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/Half.h>
#include <c10/core/TensorImpl.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <c10/util/Optional.h>
#include <ATen/core/ATenDispatch.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>



namespace at {

/* example
Tensor * MkldnnCPUType::add(Tensor & a, Tensor & b) {
  std::cout << "add Tensor with backend MkldnnCPU\n";
  return &a;
}
*/

Tensor MkldnnCPUType::add(const Tensor & self, const Tensor & other, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_add(self, other, alpha);
}
Tensor & MkldnnCPUType::add_(Tensor & self, const Tensor & other, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_add_(self, other, alpha);
}
Tensor & MkldnnCPUType::add_out(Tensor & out, const Tensor & self, const Tensor & other, Scalar alpha) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_add_out(out, self, other, alpha);
}
Tensor MkldnnCPUType::empty(IntArrayRef size, const TensorOptions & options, c10::optional<MemoryFormat> memory_format) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const DeviceGuard device_guard(options.device());
    return at::native::empty_mkldnn(size, options, memory_format);
}
Tensor MkldnnCPUType::mkldnn_linear(const Tensor & input, const Tensor & weight, const Tensor & bias) {
#ifdef BUILD_NAMEDTENSOR
    if (input.has_names() || weight.has_names() || bias.has_names()) {
        AT_ERROR("mkldnn_linear: no named inference rule implemented.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::mkldnn_linear(input, weight, bias);
}
Tensor MkldnnCPUType::mkldnn_max_pool2d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool ceil_mode) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR("mkldnn_max_pool2d: no named inference rule implemented.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_max_pool2d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
Tensor MkldnnCPUType::mul(const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_mul(self, other);
}
Tensor & MkldnnCPUType::mul_(Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_mul_(self, other);
}
Tensor & MkldnnCPUType::mul_out(Tensor & out, const Tensor & self, const Tensor & other) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_mul_out(out, self, other);
}
std::tuple<Tensor,Tensor,Tensor> MkldnnCPUType::native_batch_norm(const Tensor & input, const Tensor & weight, const Tensor & bias, const Tensor & running_mean, const Tensor & running_var, bool training, double momentum, double eps) {
#ifdef BUILD_NAMEDTENSOR
    if (input.has_names() || weight.has_names() || bias.has_names() || running_mean.has_names() || running_var.has_names()) {
        AT_ERROR("native_batch_norm: no named inference rule implemented.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(input));
    return at::native::mkldnn_batch_norm(input, weight, bias, running_mean, running_var, training, momentum, eps);
}
Tensor MkldnnCPUType::_mkldnn_reshape(const Tensor & self, IntArrayRef shape) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR("_mkldnn_reshape: no named inference rule implemented.");
    }
#endif
    // DeviceGuard omitted
    return at::native::mkldnn_reshape(self, shape);
}
Tensor MkldnnCPUType::relu(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR("relu: no named inference rule implemented.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_relu(self);
}
Tensor & MkldnnCPUType::relu_(Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_relu_(self);
}
Tensor MkldnnCPUType::sigmoid(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_sigmoid(self);
}
Tensor & MkldnnCPUType::sigmoid_(Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_sigmoid_(self);
}
Tensor MkldnnCPUType::_softmax(const Tensor & self, int64_t dim, bool half_to_float) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_softmax(self, dim, half_to_float);
}
Tensor MkldnnCPUType::_mkldnn_transpose(const Tensor & self, int64_t dim0, int64_t dim1) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR("_mkldnn_transpose: no named inference rule implemented.");
    }
#endif
    // DeviceGuard omitted
    return at::native::mkldnn_transpose(self, dim0, dim1);
}
Tensor & MkldnnCPUType::_mkldnn_transpose_(Tensor & self, int64_t dim0, int64_t dim1) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR("_mkldnn_transpose_: no named inference rule implemented.");
    }
#endif
    // DeviceGuard omitted
    return at::native::mkldnn_transpose_(self, dim0, dim1);
}
Tensor MkldnnCPUType::clone(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_clone(self);
}
Tensor & MkldnnCPUType::zero_(Tensor & self) {
#ifdef BUILD_NAMEDTENSOR

#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_zero_(self);
}
Tensor MkldnnCPUType::to_dense(const Tensor & self) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR("to_dense: no named inference rule implemented.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_to_dense(self);
}
Tensor MkldnnCPUType::mkldnn_reorder_conv2d_weight(const Tensor & self, IntArrayRef padding, IntArrayRef stride, IntArrayRef dilation, int64_t groups) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR("mkldnn_reorder_conv2d_weight: no named inference rule implemented.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_reorder_conv2d_weight(self, padding, stride, dilation, groups);
}
Tensor MkldnnCPUType::view(const Tensor & self, IntArrayRef size) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR("view: no named inference rule implemented.");
    }
#endif
    // DeviceGuard omitted
    return at::native::mkldnn_view(self, size);
}
Tensor & MkldnnCPUType::adaptive_avg_pool2d_out(Tensor & out, const Tensor & self, IntArrayRef output_size) {
#ifdef BUILD_NAMEDTENSOR
    if (out.has_names() || self.has_names()) {
        AT_ERROR("adaptive_avg_pool2d_out: no named inference rule implemented.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_adaptive_avg_pool2d_out(out, self, output_size);
}
Tensor MkldnnCPUType::mkldnn_adaptive_avg_pool2d(const Tensor & self, IntArrayRef output_size) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR("mkldnn_adaptive_avg_pool2d: no named inference rule implemented.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_adaptive_avg_pool2d(self, output_size);
}
Tensor & MkldnnCPUType::avg_pool2d_out(Tensor & out, const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
#ifdef BUILD_NAMEDTENSOR
    if (out.has_names() || self.has_names()) {
        AT_ERROR("avg_pool2d_out: no named inference rule implemented.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_avg_pool2d_out(out, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
Tensor MkldnnCPUType::avg_pool2d(const Tensor & self, IntArrayRef kernel_size, IntArrayRef stride, IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
#ifdef BUILD_NAMEDTENSOR
    if (self.has_names()) {
        AT_ERROR("avg_pool2d: no named inference rule implemented.");
    }
#endif
    const OptionalDeviceGuard device_guard(device_of(self));
    return at::native::mkldnn_avg_pool2d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}

static auto& registerer = globalATenDispatch()
  .registerOp<Tensor (const Tensor &, const Tensor &, Scalar)>(Backend::MkldnnCPU, "aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor", &MkldnnCPUType::add)
  .registerOp<Tensor & (Tensor &, const Tensor &, Scalar)>(Backend::MkldnnCPU, "aten::add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)", &MkldnnCPUType::add_)
  .registerOp<Tensor & (Tensor &, const Tensor &, const Tensor &, Scalar)>(Backend::MkldnnCPU, "aten::add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)", &MkldnnCPUType::add_out)
  .registerOp<Tensor (IntArrayRef, const TensorOptions &, c10::optional<MemoryFormat>)>(Backend::MkldnnCPU, "aten::empty.memory_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor", &MkldnnCPUType::empty)
  .registerOp<Tensor (const Tensor &, const Tensor &, const Tensor &)>(Backend::MkldnnCPU, "aten::mkldnn_linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor", &MkldnnCPUType::mkldnn_linear)
  .registerOp<Tensor (const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, IntArrayRef, bool)>(Backend::MkldnnCPU, "aten::mkldnn_max_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> Tensor", &MkldnnCPUType::mkldnn_max_pool2d)
  .registerOp<Tensor (const Tensor &, const Tensor &)>(Backend::MkldnnCPU, "aten::mul.Tensor(Tensor self, Tensor other) -> Tensor", &MkldnnCPUType::mul)
  .registerOp<Tensor & (Tensor &, const Tensor &)>(Backend::MkldnnCPU, "aten::mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)", &MkldnnCPUType::mul_)
  .registerOp<Tensor & (Tensor &, const Tensor &, const Tensor &)>(Backend::MkldnnCPU, "aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)", &MkldnnCPUType::mul_out)
  .registerOp<std::tuple<Tensor,Tensor,Tensor> (const Tensor &, const Tensor &, const Tensor &, const Tensor &, const Tensor &, bool, double, double)>(Backend::MkldnnCPU, "aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)", &MkldnnCPUType::native_batch_norm)
  .registerOp<Tensor (const Tensor &, IntArrayRef)>(Backend::MkldnnCPU, "aten::_mkldnn_reshape(Tensor self, int[] shape) -> Tensor", &MkldnnCPUType::_mkldnn_reshape)
  .registerOp<Tensor (const Tensor &)>(Backend::MkldnnCPU, "aten::relu(Tensor self) -> Tensor", &MkldnnCPUType::relu)
  .registerOp<Tensor & (Tensor &)>(Backend::MkldnnCPU, "aten::relu_(Tensor(a!) self) -> Tensor(a!)", &MkldnnCPUType::relu_)
  .registerOp<Tensor (const Tensor &)>(Backend::MkldnnCPU, "aten::sigmoid(Tensor self) -> Tensor", &MkldnnCPUType::sigmoid)
  .registerOp<Tensor & (Tensor &)>(Backend::MkldnnCPU, "aten::sigmoid_(Tensor(a!) self) -> Tensor(a!)", &MkldnnCPUType::sigmoid_)
  .registerOp<Tensor (const Tensor &, int64_t, bool)>(Backend::MkldnnCPU, "aten::_softmax(Tensor self, int dim, bool half_to_float) -> Tensor", &MkldnnCPUType::_softmax)
  .registerOp<Tensor (const Tensor &, int64_t, int64_t)>(Backend::MkldnnCPU, "aten::_mkldnn_transpose(Tensor self, int dim0, int dim1) -> Tensor", &MkldnnCPUType::_mkldnn_transpose)
  .registerOp<Tensor & (Tensor &, int64_t, int64_t)>(Backend::MkldnnCPU, "aten::_mkldnn_transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)", &MkldnnCPUType::_mkldnn_transpose_)
  .registerOp<Tensor (const Tensor &)>(Backend::MkldnnCPU, "aten::clone(Tensor self) -> Tensor", &MkldnnCPUType::clone)
  .registerOp<Tensor & (Tensor &)>(Backend::MkldnnCPU, "aten::zero_(Tensor(a!) self) -> Tensor(a!)", &MkldnnCPUType::zero_)
  .registerOp<Tensor (const Tensor &)>(Backend::MkldnnCPU, "aten::to_dense(Tensor self) -> Tensor", &MkldnnCPUType::to_dense)
  .registerOp<Tensor (const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, int64_t)>(Backend::MkldnnCPU, "aten::mkldnn_reorder_conv2d_weight(Tensor self, int[2] padding=0, int[2] stride=1, int[2] dilation=1, int groups=1) -> Tensor", &MkldnnCPUType::mkldnn_reorder_conv2d_weight)
  .registerOp<Tensor (const Tensor &, IntArrayRef)>(Backend::MkldnnCPU, "aten::view(Tensor(a) self, int[] size) -> Tensor(a)", &MkldnnCPUType::view)
  .registerOp<Tensor & (Tensor &, const Tensor &, IntArrayRef)>(Backend::MkldnnCPU, "aten::adaptive_avg_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)", &MkldnnCPUType::adaptive_avg_pool2d_out)
  .registerOp<Tensor (const Tensor &, IntArrayRef)>(Backend::MkldnnCPU, "aten::mkldnn_adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor", &MkldnnCPUType::mkldnn_adaptive_avg_pool2d)
  .registerOp<Tensor & (Tensor &, const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, bool, bool, c10::optional<int64_t>)>(Backend::MkldnnCPU, "aten::avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)", &MkldnnCPUType::avg_pool2d_out)
  .registerOp<Tensor (const Tensor &, IntArrayRef, IntArrayRef, IntArrayRef, bool, bool, c10::optional<int64_t>)>(Backend::MkldnnCPU, "aten::avg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor", &MkldnnCPUType::avg_pool2d);
}
